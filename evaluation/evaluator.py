def evaluate_matches(predictions: list[dict], ground_truth: list[dict] | None = None) -> dict:
    """
    Evaluate matching performance.

    Args:
        predictions (list[dict])
        ground_truth (optional)

    Returns:
        dict: Evaluation metrics
    """
    pass
